<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Robust Multimodal Learning via Cross-Modal Proxy Tokens">
    <meta name="author" content="Md Kaykobad Reza, Ameya Patil, Mashhour Solh, M. Salman Asif">

    <title>Robust Multimodal Learning via Cross-Modal Proxy Tokens</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="src/css/style.css">
</head>

<body>

<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Robust Multimodal Learning via</h2>
    <h2>Cross-Modal Proxy Tokens</h2>
        <p class="abstract"><b>Learning to approximate missing modality features efficiently</b></p>
    <hr>
    <p class="authors">
        <a href="https://kaykobad.github.io/" target="_blank">Md Kaykobad Reza<sup> 1</sup></a>,
        <a href="https://scholar.google.com/citations?user=tCjPO0oAAAAJ&hl=en" target="_blank">Ameya Patil<sup> 2</sup></a>,
        <a href="https://scholar.google.com/citations?user=_2uT_RAAAAAJ&hl=en" target="_blank">Mashhour Solh<sup> 2</sup></a>, and
        <a href="https://intra.ece.ucr.edu/~sasif/" target="_blank"> M. Salman Asif<sup> 1</sup></a>
    </p>
    <p>
        <a><sup>1</sup> University of California Riverside, CA, USA</a></br>
        <a><sup>2</sup> Amazon, CA, USA</a></br>
    </p>

    <div>
        <a class="btn btn-primary" href="https://openreview.net/forum?id=Wtc6wvcYJ0" target="_blank">Paper (Open Review)</a> 
        <a class="btn btn-primary" href="https://arxiv.org/abs/2501.17823" target="_blank">Paper (arXiv)</a> 
        <a class="btn btn-primary" href="https://github.com/CSIPlab/Cross-Modal-Proxy-Tokens" target="_blank">Code (GitHub)</a>
        <a class="btn btn-primary" href="https://csiplab.github.io/cross-modal-proxy-tokens/">Webpage</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>Multimodal models often experience a significant performance drop when one or more modalities are missing during inference. To address this challenge, we propose a simple yet effective approach that enhances robustness to missing modalities while maintaining strong performance when all modalities are available. Our method introduces cross-modal proxy tokens (CMPTs), which approximate the class token of a missing modality by attending only to the tokens of the available modality without requiring explicit modality generation or auxiliary networks. To efficiently learn these approximations with minimal computational overhead, we employ low-rank adapters in frozen unimodal encoders and jointly optimize an alignment loss with a task-specific loss. Extensive experiments on five multimodal datasets show that our method outperforms state-of-the-art baselines across various missing rates while achieving competitive results in complete-modality settings. Overall, our method offers a flexible and efficient solution for robust multimodal learning.</p>
        <br>

        <div class="row">
            <div class="col text-center">
                <img src="./img/model-cmpt.png" style="width:90%" alt="Banner">
                <p class="text-left"><b>Figure 1:</b> (a) We introduce Cross-Modal Proxy Tokens (CMPTs), a novel approach to address missing modality
                challenges. CMPTs effectively learn to approximate missing modality class tokens by adapting pretrained encoders
                through a joint optimization of alignment and task-specific objectives. Our approach accommodates both complete
                and missing modalities during training and inference, thereby enhancing robustness across varying missing modality
                scenarios. (b) CMPTs achieve state-of-the-art performance, consistently outperforming recent baseline methods in
                both complete and missing modality scenarios. The radar plot illustrates F1-macro scores on the MM-IMDb dataset across varying modality availability.</p>
            </div>
        </div>

        <!-- <video width="100%" height="400" controls="True" preload="none" loop muted autoplay = 'autoplay' playsInline>
            <source type="video/mp4" src="img/Presentation1.mov" />
            <source type="video/webm" src="img/video1.webm" />
        </video> -->
    </div>

     <!-- CONTRIBUTIONS -->
    <div class="section">
        <h2>Contributions</h2>
        <hr>
        <ul>
        <li>Introduced <b>Cross-Modal Proxy Tokens (CMPTs)</b> to approximate the class token of the missing modality from the available modality.</li>
        <li>Used <b>rank-1 Low-Rank Adapters (LoRA)</b> for efficient adaptation of frozen unimodal encoders.</li>
        <li>Achieved <b>state-of-the-art robustness</b> across five datasets and 12 baselines under different missing-modality conditions.</li>
        <li>Maintained <b>competitive or superior performance</b> when all modalities are available.</li>
        <li>Demonstrated <b>class-level improvements</b> and better generalization to unseen missing scenarios during inference.</li>
        </ul>
    </div>

    <!-- RESULTS -->
    <div class="section">
        <h2>Key Results</h2>
        <hr>
        <div class="row text-center">
        <div class="col-md-12">
            <img src="./img/table1.png" style="width:90%" alt="Performance under missing modalities">
            <p class="text-left"><b>Table 1:</b> CMPTs outperform existing baselines across various missing-modality configurations on MM-IMDb and UPMC Food-101 datasets.</p>
        </div>
        <div class="col-md-12">
            <img src="./img/table11.png" style="width:90%" alt="Performance with all modalities">
            <p class="text-left"><b>Tables 2:</b> CMPTs outperform existing baselines even when a complete modality gets missing during inference.</p>
        </div>
        </div>
    </div>


    <!-- GENERALIZATION -->
    <div class="section">
        <h2>Generalization and Robustness</h2>
        <hr>
        <div class="row text-center">
        <div class="col-md-12">
            <img src="./img/fig1.png" style="width:90%" alt="Generalization under missing text">
            <p class="text-left"><b>Figure 1:</b> Models are trained with 100% image + 100% text and evaluated with 100% image + x% text. CMPTs generalize effectively across varying missing-modality rates, retaining strong performance even with only 10% text available.</p>
        </div>
        <div class="col-md-12">
            <img src="./img/fig2.png" style="width:90%" alt="Per-class robustness">
            <p class="text-left"><b>Figure 2:</b> Per-class improvement on MM-IMDb. CMPTs enhance performance across most classes, especially modality-sensitive ones (e.g., Horror, Animation).</p>
        </div>
        </div>
    </div>

    <!-- QUALITATIVE -->
    <div class="section">
        <h2>Qualitative Analysis</h2>
        <hr>
        <div class="row text-center">
        <div class="col-md-12">
            <img src="./img/tsne.png" style="width:90%" alt="t-SNE visualization">
            <p class="text-left"><b>Figure 3:</b> t-SNE visualization of fused embeddings. CMPTs (blue) align missing-modality features closely with full-modality embeddings (green).</p>
        </div>
        <div class="col-md-12">
            <img src="./img/attention-map.png" style="width:90%" alt="Attention visualization">
            <p class="text-left"><b>Figure 4:</b> Attention maps show CMPTs focus on semantically relevant regions, correcting mispredictions caused by missing modalities.</p>
        </div>
        </div>
    </div>

    <!-- FUTURE WORK -->
    <div class="section">
        <h2>Future Directions</h2>
        <hr>
        <ul>
        <li>Design a better training strategy that encourages more uniform reliance on different modalities while maintaining overall performance in both complete and missing modality scenarios.</li>
        <li>Develop a general framework for explaining modality contribution and expected performance drop under different missing modality scenarios.</li>
        <li>Extend CMPTs to three or more modalities to enhance generalizability and applicability to a wide range of datasets and tasks.</li>
        </ul>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
  @article{reza2025robust,
    title={Robust Multimodal Learning via Cross-Modal Proxy Tokens},
    author={Reza, Md Kaykobad and Patil, Ameya and Solh, Mashhour and Asif, M Salman},
    journal={arXiv preprint arXiv:2501.17823},
    year={2025}
  }
        </div>
    </div>

    <hr>
</div>

</body>
</html>
